//
// AUTO-GENERATED FILE, DO NOT MODIFY!
//
// @dart=2.12

// ignore_for_file: unused_element, unused_import
// ignore_for_file: always_put_required_named_parameters_first
// ignore_for_file: constant_identifier_names
// ignore_for_file: lines_longer_than_80_chars

part of openapi.api;

class CreateChatCompletionRequest {
  /// Returns a new [CreateChatCompletionRequest] instance.
  CreateChatCompletionRequest({
    this.messages = const [],
    required this.model,
    this.frequencyPenalty = 0,
    this.logitBias = const {},
    this.logprobs = false,
    this.topLogprobs,
    this.maxTokens,
    this.n = 1,
    this.presencePenalty = 0,
    this.responseFormat,
    this.seed,
    this.stop,
    this.stream = false,
    this.temperature = 1,
    this.topP = 1,
    this.tools = const [],
    this.toolChoice,
    this.user,
    this.functionCall,
    this.functions = const [],
  });

  /// A list of messages comprising the conversation so far. [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models).
  List<ChatCompletionRequestMessage> messages;

  CreateChatCompletionRequestModel model;

  /// Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.  [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details) 
  ///
  /// Minimum value: -2
  /// Maximum value: 2
  num? frequencyPenalty;

  /// Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. 
  Map<String, int>? logitBias;

  /// Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`. This option is currently not available on the `gpt-4-vision-preview` model.
  bool? logprobs;

  /// An integer between 0 and 5 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.
  ///
  /// Minimum value: 0
  /// Maximum value: 5
  int? topLogprobs;

  /// The maximum number of [tokens](/tokenizer) that can be generated in the chat completion.  The total length of input tokens and generated tokens is limited by the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens. 
  int? maxTokens;

  /// How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.
  ///
  /// Minimum value: 1
  /// Maximum value: 128
  int? n;

  /// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.  [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details) 
  ///
  /// Minimum value: -2
  /// Maximum value: 2
  num? presencePenalty;

  ///
  /// Please note: This property should have been non-nullable! Since the specification file
  /// does not include a default value (using the "default:" property), however, the generated
  /// source code must fall back to having a nullable type.
  /// Consider adding a "default:" property in the specification file to hide this note.
  ///
  CreateChatCompletionRequestResponseFormat? responseFormat;

  /// This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend. 
  ///
  /// Minimum value: -9223372036854775808
  /// Maximum value: 9223372036854775807
  int? seed;

  ///
  /// Please note: This property should have been non-nullable! Since the specification file
  /// does not include a default value (using the "default:" property), however, the generated
  /// source code must fall back to having a nullable type.
  /// Consider adding a "default:" property in the specification file to hide this note.
  ///
  CreateChatCompletionRequestStop? stop;

  /// If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions). 
  bool? stream;

  /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.  We generally recommend altering this or `top_p` but not both. 
  ///
  /// Minimum value: 0
  /// Maximum value: 2
  num? temperature;

  /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or `temperature` but not both. 
  ///
  /// Minimum value: 0
  /// Maximum value: 1
  num? topP;

  /// A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. 
  List<ChatCompletionTool> tools;

  ///
  /// Please note: This property should have been non-nullable! Since the specification file
  /// does not include a default value (using the "default:" property), however, the generated
  /// source code must fall back to having a nullable type.
  /// Consider adding a "default:" property in the specification file to hide this note.
  ///
  ChatCompletionToolChoiceOption? toolChoice;

  /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids). 
  ///
  /// Please note: This property should have been non-nullable! Since the specification file
  /// does not include a default value (using the "default:" property), however, the generated
  /// source code must fall back to having a nullable type.
  /// Consider adding a "default:" property in the specification file to hide this note.
  ///
  String? user;

  ///
  /// Please note: This property should have been non-nullable! Since the specification file
  /// does not include a default value (using the "default:" property), however, the generated
  /// source code must fall back to having a nullable type.
  /// Consider adding a "default:" property in the specification file to hide this note.
  ///
  CreateChatCompletionRequestFunctionCall? functionCall;

  /// Deprecated in favor of `tools`.  A list of functions the model may generate JSON inputs for. 
  List<ChatCompletionFunctions> functions;

  @override
  bool operator ==(Object other) => identical(this, other) || other is CreateChatCompletionRequest &&
    _deepEquality.equals(other.messages, messages) &&
    other.model == model &&
    other.frequencyPenalty == frequencyPenalty &&
    _deepEquality.equals(other.logitBias, logitBias) &&
    other.logprobs == logprobs &&
    other.topLogprobs == topLogprobs &&
    other.maxTokens == maxTokens &&
    other.n == n &&
    other.presencePenalty == presencePenalty &&
    other.responseFormat == responseFormat &&
    other.seed == seed &&
    other.stop == stop &&
    other.stream == stream &&
    other.temperature == temperature &&
    other.topP == topP &&
    _deepEquality.equals(other.tools, tools) &&
    other.toolChoice == toolChoice &&
    other.user == user &&
    other.functionCall == functionCall &&
    _deepEquality.equals(other.functions, functions);

  @override
  int get hashCode =>
    // ignore: unnecessary_parenthesis
    (messages.hashCode) +
    (model.hashCode) +
    (frequencyPenalty == null ? 0 : frequencyPenalty!.hashCode) +
    (logitBias == null ? 0 : logitBias!.hashCode) +
    (logprobs == null ? 0 : logprobs!.hashCode) +
    (topLogprobs == null ? 0 : topLogprobs!.hashCode) +
    (maxTokens == null ? 0 : maxTokens!.hashCode) +
    (n == null ? 0 : n!.hashCode) +
    (presencePenalty == null ? 0 : presencePenalty!.hashCode) +
    (responseFormat == null ? 0 : responseFormat!.hashCode) +
    (seed == null ? 0 : seed!.hashCode) +
    (stop == null ? 0 : stop!.hashCode) +
    (stream == null ? 0 : stream!.hashCode) +
    (temperature == null ? 0 : temperature!.hashCode) +
    (topP == null ? 0 : topP!.hashCode) +
    (tools.hashCode) +
    (toolChoice == null ? 0 : toolChoice!.hashCode) +
    (user == null ? 0 : user!.hashCode) +
    (functionCall == null ? 0 : functionCall!.hashCode) +
    (functions.hashCode);

  @override
  String toString() => 'CreateChatCompletionRequest[messages=$messages, model=$model, frequencyPenalty=$frequencyPenalty, logitBias=$logitBias, logprobs=$logprobs, topLogprobs=$topLogprobs, maxTokens=$maxTokens, n=$n, presencePenalty=$presencePenalty, responseFormat=$responseFormat, seed=$seed, stop=$stop, stream=$stream, temperature=$temperature, topP=$topP, tools=$tools, toolChoice=$toolChoice, user=$user, functionCall=$functionCall, functions=$functions]';

  Map<String, dynamic> toJson() {
    final json = <String, dynamic>{};
      json[r'messages'] = this.messages;
      json[r'model'] = this.model;
    if (this.frequencyPenalty != null) {
      json[r'frequency_penalty'] = this.frequencyPenalty;
    } else {
      json[r'frequency_penalty'] = null;
    }
    if (this.logitBias != null) {
      json[r'logit_bias'] = this.logitBias;
    } else {
      json[r'logit_bias'] = null;
    }
    if (this.logprobs != null) {
      json[r'logprobs'] = this.logprobs;
    } else {
      json[r'logprobs'] = null;
    }
    if (this.topLogprobs != null) {
      json[r'top_logprobs'] = this.topLogprobs;
    } else {
      json[r'top_logprobs'] = null;
    }
    if (this.maxTokens != null) {
      json[r'max_tokens'] = this.maxTokens;
    } else {
      json[r'max_tokens'] = null;
    }
    if (this.n != null) {
      json[r'n'] = this.n;
    } else {
      json[r'n'] = null;
    }
    if (this.presencePenalty != null) {
      json[r'presence_penalty'] = this.presencePenalty;
    } else {
      json[r'presence_penalty'] = null;
    }
    if (this.responseFormat != null) {
      json[r'response_format'] = this.responseFormat;
    } else {
      json[r'response_format'] = null;
    }
    if (this.seed != null) {
      json[r'seed'] = this.seed;
    } else {
      json[r'seed'] = null;
    }
    if (this.stop != null) {
      json[r'stop'] = this.stop;
    } else {
      json[r'stop'] = null;
    }
    if (this.stream != null) {
      json[r'stream'] = this.stream;
    } else {
      json[r'stream'] = null;
    }
    if (this.temperature != null) {
      json[r'temperature'] = this.temperature;
    } else {
      json[r'temperature'] = null;
    }
    if (this.topP != null) {
      json[r'top_p'] = this.topP;
    } else {
      json[r'top_p'] = null;
    }
      json[r'tools'] = this.tools;
    if (this.toolChoice != null) {
      json[r'tool_choice'] = this.toolChoice;
    } else {
      json[r'tool_choice'] = null;
    }
    if (this.user != null) {
      json[r'user'] = this.user;
    } else {
      json[r'user'] = null;
    }
    if (this.functionCall != null) {
      json[r'function_call'] = this.functionCall;
    } else {
      json[r'function_call'] = null;
    }
      json[r'functions'] = this.functions;
    return json;
  }

  /// Returns a new [CreateChatCompletionRequest] instance and imports its values from
  /// [value] if it's a [Map], null otherwise.
  // ignore: prefer_constructors_over_static_methods
  static CreateChatCompletionRequest? fromJson(dynamic value) {
    if (value is Map) {
      final json = value.cast<String, dynamic>();

      // Ensure that the map contains the required keys.
      // Note 1: the values aren't checked for validity beyond being non-null.
      // Note 2: this code is stripped in release mode!
      assert(() {
        requiredKeys.forEach((key) {
          assert(json.containsKey(key), 'Required key "CreateChatCompletionRequest[$key]" is missing from JSON.');
          assert(json[key] != null, 'Required key "CreateChatCompletionRequest[$key]" has a null value in JSON.');
        });
        return true;
      }());

      return CreateChatCompletionRequest(
        messages: ChatCompletionRequestMessage.listFromJson(json[r'messages']),
        model: CreateChatCompletionRequestModel.fromJson(json[r'model'])!,
        frequencyPenalty: json[r'frequency_penalty'] == null
            ? 0
            : num.parse('${json[r'frequency_penalty']}'),
        logitBias: mapCastOfType<String, int>(json, r'logit_bias') ?? const {},
        logprobs: mapValueOfType<bool>(json, r'logprobs') ?? false,
        topLogprobs: mapValueOfType<int>(json, r'top_logprobs'),
        maxTokens: mapValueOfType<int>(json, r'max_tokens'),
        n: mapValueOfType<int>(json, r'n') ?? 1,
        presencePenalty: json[r'presence_penalty'] == null
            ? 0
            : num.parse('${json[r'presence_penalty']}'),
        responseFormat: CreateChatCompletionRequestResponseFormat.fromJson(json[r'response_format']),
        seed: mapValueOfType<int>(json, r'seed'),
        stop: CreateChatCompletionRequestStop.fromJson(json[r'stop']),
        stream: mapValueOfType<bool>(json, r'stream') ?? false,
        temperature: json[r'temperature'] == null
            ? 1
            : num.parse('${json[r'temperature']}'),
        topP: json[r'top_p'] == null
            ? 1
            : num.parse('${json[r'top_p']}'),
        tools: ChatCompletionTool.listFromJson(json[r'tools']),
        toolChoice: ChatCompletionToolChoiceOption.fromJson(json[r'tool_choice']),
        user: mapValueOfType<String>(json, r'user'),
        functionCall: CreateChatCompletionRequestFunctionCall.fromJson(json[r'function_call']),
        functions: ChatCompletionFunctions.listFromJson(json[r'functions']),
      );
    }
    return null;
  }

  static List<CreateChatCompletionRequest> listFromJson(dynamic json, {bool growable = false,}) {
    final result = <CreateChatCompletionRequest>[];
    if (json is List && json.isNotEmpty) {
      for (final row in json) {
        final value = CreateChatCompletionRequest.fromJson(row);
        if (value != null) {
          result.add(value);
        }
      }
    }
    return result.toList(growable: growable);
  }

  static Map<String, CreateChatCompletionRequest> mapFromJson(dynamic json) {
    final map = <String, CreateChatCompletionRequest>{};
    if (json is Map && json.isNotEmpty) {
      json = json.cast<String, dynamic>(); // ignore: parameter_assignments
      for (final entry in json.entries) {
        final value = CreateChatCompletionRequest.fromJson(entry.value);
        if (value != null) {
          map[entry.key] = value;
        }
      }
    }
    return map;
  }

  // maps a json object with a list of CreateChatCompletionRequest-objects as value to a dart map
  static Map<String, List<CreateChatCompletionRequest>> mapListFromJson(dynamic json, {bool growable = false,}) {
    final map = <String, List<CreateChatCompletionRequest>>{};
    if (json is Map && json.isNotEmpty) {
      // ignore: parameter_assignments
      json = json.cast<String, dynamic>();
      for (final entry in json.entries) {
        map[entry.key] = CreateChatCompletionRequest.listFromJson(entry.value, growable: growable,);
      }
    }
    return map;
  }

  /// The list of required keys that must be present in a JSON.
  static const requiredKeys = <String>{
    'messages',
    'model',
  };
}

